---
title: "MovieLens project for Data Science Capstone Course"
author: "Shahidul Islam"
date: "2024-11-29"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The purpose of this project is to wrangle the MovieLens dataset, describe its characteristics, and predict movie ratings based on features such as users, movies, and genres. Analytic datasets were created following the instructions provided in the course materials. The MovieLens dataset was downloaded and curated in accordance with the provided guidelines. A final hold-out test dataset was generated by partitioning the main dataset and extracting 10% of the MovieLens data using the "caret" package.

The training dataset was explored and analyzed using tools from the "tidyverse" and "sqldf" packages, focusing on distributions, counts, outliers, and other key features. Several models were developed to predict movie ratings, with complexity introduced gradually. Root Mean Square Error (RMSE) was calculated for each model and recorded in a data frame for reporting. The RMSE for the best model was 0.86445. The algrorithm for the RMSE:

$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2}
$$

## Dataset
The MovieLens 10M datasets was splitted into 90% training and 10% validation datasets. The training dataset included 69878 unique users with 9000055 observations (ratings), and 10677 unique movies. The validation dataset "final_holdout_test" included 999999 ratings from 68534 unique users. This validation dataset only used for prediction, i.e., evaluating RMSE, it was not used  to train any algorithm.

```{r load the neccessary packages}
library(tidyverse)
library(caret)
library(sqldf)

```

```{r edx and final_holdout_test datasets, eval=FALSE }
##########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

# if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
# if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# library(tidyverse)
# library(caret)
# library(sqldf)

setwd("C:/Users/ShahS/OneDrive/EDX.ORG/DataScienceCertificate/CAPSTONE")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), 
                                   simplify = TRUE), stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```



## Exploring the edx dataset

To better understand the data set, we can print a few rows, examine its dimensions, inspect the variable types, and perform other exploratory checks.


## *******We created the 'edx' and 'final_holdout_test' (Validation) data sets using the code above and saved them locally, as generating these data sets can be time-consuming. By loading the data from the local drive, we were able to significantly improve run time.********

```{r data explore, echo=TRUE}
setwd("C:/Users/ShahS/OneDrive/EDX.ORG/DataScienceCertificate/CAPSTONE")
load(file="edx.Rdata")
load(file="final_holdout_test.Rdata")

head(edx, n=5)
names(edx)
str(edx)
#69878 unique users have  9000055 observations in the dataset. 10677
sqldf("select count(distinct userId) from edx")
sqldf("select count( movieId) from edx")

#68534 unique users have 999999 observations
sqldf("select count(distinct userId) from final_holdout_test")
```
 
## Top 5 higest rated movies overall
```{r high rated movies}
edx %>%
  group_by(title) %>%
  summarize(N_ratings = n(), avg_ratings = mean(rating, na.rm = TRUE)) %>%
  arrange(desc(avg_ratings)) %>%
  slice_max(order_by = avg_ratings, n = 5)

```

##Top 5 highest rated movies with >1000 user ratings

```{r highest rated movies w/>1000 user ratings}
edx %>%
  group_by(title) %>%
  summarize(N_ratings = n(), avg_ratings = mean(rating, na.rm = TRUE)) %>%
  filter(N_ratings>1000)%>%
  arrange(desc(avg_ratings)) %>%
  slice_max(order_by = avg_ratings, n = 5)

```
##cleaning  the genres variable to parse values of genres--each movie is >=1 genres, let's ensure we get the count by each genres

```{r cleaning up generes }
edx %>% 
  separate_rows(genres, sep = "\\|") %>% 
  group_by(genres) %>% 
  summarize(N_ratings = n()) %>% 
  arrange(desc(N_ratings)) %>% 
  slice_max(order_by = N_ratings, n = 5)

```

## Distribution of ratings
Based on the distribution below, a rating of 4 is the most common, followed by 3 and 5. It is evident that some movies are rated by significantly more users than others, while some movies receive very few ratings. This type of non-normal distribution may not generalize well, potentially leading to suboptimal estimates.

To address this issue, we will later implement regularization techniques, which help reduce error by adding a penalty term to the model. This approach mitigates overfitting by preventing the model coefficients from taking on extreme values. As a result, introducing regularization leads to more robust and accurate models.

```{r distribution of ratings }
ggplot(edx, aes(x = rating)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Ratings", x = "Values", y = "Frequency") 
```
## Checking for outliers

List of movies that received one rating only. These outliers may influence predictions
```{r outliers}
edx %>%
  group_by(movieId) %>%
  summarize(ratings = n()) %>%
  filter(ratings == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = mean(rating), n_rating = n()) %>%
  #slice(1:100) %>%
  knitr::kable()
```


## looking at the distribution of users

```{r user distribution}
edx %>% 
  count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "cyan") +
  ggtitle("Distribution of Users Who Rated Movies in Original Scale") +
  scale_x_continuous(
    breaks = c(0,30, 50, 100, 200, 500, 1000, 2000, 3000, 4000, 5000, 6000), # Exact breakpoints
    labels = c(0, 30, 50, 100, 200, 500, 1000, 2000, 3000, 4000, 5000, 6000), # Matching labels
    limits = c(0, 6000), # Ensure the x-axis range
    expand = c(0, 0) # Remove extra padding around the axis
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels if needed

```

## Distribution of users in a log scale to get better view of the distribution
```{r distribution in log scale}
edx %>% 
  count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "cyan") +
  ggtitle("Distribution of Users Who Rated Movies in Log Scale") +
  scale_x_log10(
    breaks = c(1, 10, 20, 30, 50, 100, 200, 500, 1000), # Logarithmic breakpoints
    labels = c(1, 10, 20, 30, 50, 100, 200, 500, 1000)  # Matching labels
  )

```

## Histogram showing the distribution of average movie ratings
##given by users who have rated more than 1000 movies.

```{r users rated more than 1000 movies}
edx%>%group_by(userId)%>%
  filter(n()>1000)%>%
  summarize(mean_rating=mean(rating))%>%
  ggplot(aes(mean_rating))+
  geom_histogram(bins=40, color="cyan")+
  xlab("Average rating")+
  ylab("Count of users")+
  ggtitle("Mean ratings by # of users")+
  scale_x_continuous(
    breaks = c(0,1, 2, 3, 4, 5, 6), # Exact breakpoints
    labels = c(0,1, 2, 3, 4, 5, 6) # Matching labels
  )
```
## Predictive Models#####################################

Here is equation that computes RMSE, which evaluates the models. Lower RMSE suggests better model.
$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2}
$$

##Function to compute RMSE
```{r RMSE function}
RMSE <-  function(true_ratings, predicted_ratings){ 
  sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

## 1. Naive Model to estimate mean movie ratings
This is the base model. It simply estimates the average of overall ratings

$$Y_{u,i} = \mu + \epsilon_{u,i}$$
So, here is the evarage of all ratings:
```{r Naive Model}
mu<-mean(edx$rating)
mu
# RMSE for the naive model
validation<-final_holdout_test
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse

# create dataframe to store RMSE
rmse_results <- as.data.frame(tibble(Model ="Naive Average movie rating model", 
                                     RMSE = naive_rmse))
rmse_results %>% knitr::kable()
```

## 2. Movie effect model
As observed earlier, some movies received high ratings while others were rated much lower. To account for this variability, we compute a bias term, which is the difference between each movie's ratings and the overall mean, and then calculate the average of these bias values. We plotted the distribution of the bias and subsequently incorporated the movie effect (i.e., bias) into the model.

$$
Y_{u,i} = \mu + b_i + \epsilon_{u,i}
$$


```{r Movie effect model}
# estimating bias for movie ratings
movie_avg <- edx %>%
  group_by(movieId) %>% 
  summarize(bias = mean(rating - mu))

# examine distribution of bias
movie_avg %>% 
  ggplot(aes(bias)) +
  geom_histogram(bins = 50, color = "cyan") +
  ggtitle("Distribution of bias")

# Compute the movie based predicted ratings on validation dataset
rmse_movie_model <- validation %>%
  left_join(movie_avg, by='movieId') %>%
  mutate(pred = mu + bias) %>%
  pull(pred) #Extracts the pred column as a vector.

rmse_movieBased<- RMSE(validation$rating, rmse_movie_model)
rmse_results <- rmse_results %>% add_row(Model="Movie-Based Model", 
                                         RMSE=rmse_movieBased)
rmse_results
```
 Adding the movie effect improved the RMSE compared the Naive Model.

### 3.Movie+User effect model

We wanted to see if the model could be improved further by incorporating user effect as some users rated more movies than others.

$$
Y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}
$$


```{r Mover+User model}
user_avg <- edx %>%
  left_join(movie_avg, by='movieId') %>%
  group_by(userId) %>%
  summarize(bias_user = mean(rating - mu - bias))

# Compute the predicted ratings on validation dataset (movie+user)
rmse_movie_user_model <- validation %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  mutate(pred = mu+ bias + bias_user) %>%
  pull(pred)

rmse_movie_userbased <- RMSE(validation$rating, rmse_movie_user_model)
rmse_results <- rmse_results %>% add_row(Model="Movie+User Based Model", 
                                         RMSE=rmse_movie_userbased)
rmse_results
```
Adding user effect improved the RMSE again. We now want to see if incoporating differnt genere into to the mdoel further improve RMSE.


### 4.Movie+user+Genre based model
$$
Y_{u,i} = \mu + b_i + b_u + +b_g+\epsilon_{u,i}
$$

```{r Movie+User_Genere model}
genre_pop <- edx %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  group_by(genres) %>%
  summarize(bias_user_genre = mean(rating - mu - bias - bias_user))

# Compute the predicted ratings on validation dataset

rmse_movie_user_genre_model <- validation %>%
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  left_join(genre_pop, by='genres') %>%
  mutate(pred = mu + bias + bias_user + bias_user_genre) %>%
  pull(pred)

rmse_movie_user_genre <- RMSE(validation$rating, rmse_movie_user_genre_model)
rmse_results <- rmse_results %>% add_row(Model="Movie+User+Genre Based Model", 
                                         RMSE=rmse_movie_user_genre)
rmse_results
```
Adding genre reduced RMSE slightly, not a significant improvement. 

##Regularization models########################################################
So far, we have been working with simpler models. However, the data shows significant variability in ratings. For instance, many movies were rated by only one user, while some users rated numerous movies. This variability introduces uncertainty into the data, which can make predictions less reliable.

To address this uncertainty and improve the robustness of our predictions, we will use regularization. Regularization is a modeling technique that enhances a modelâ€™s generalizability and prevents overfitting by penalizing large coefficients. This penalty discourages the model from relying too heavily on any single feature, leading to more balanced and reliable predictions.

Next, we will apply regularization techniques to all the previously developed models and evaluate their performance using the RMSE metric.


## 5.Regularization model--movie based
```{r regularization model-movie}
lambdas <- seq(0, 10, 0.1)
# Compute the predicted ratings on validation dataset using different values of lambda
rmses <- sapply(lambdas, function(lambda) {
  
  # Calculate the average by movieId
    b_i <- edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + lambda))
  
  #predicted ratings on validation data set
    predicted_ratings <- validation %>%
    left_join(b_i, by='movieId') %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  
  # Predict RMSE on validation set
    return(RMSE(validation$rating, predicted_ratings))
})

# lambda value that minimizes the RMSE
min_lambda <- lambdas[which.min(rmses)]
# Predict the RMSE on the validation set
rmse_regularized_movie <- min(rmses)
# Add the results to the results dataset
rmse_results <- rmse_results %>% add_row(Model="Regularization Movie-Based Model", RMSE=rmse_regularized_movie)
rmse_results

ggplot(data = data.frame(lambdas, rmses), aes(x = lambdas, y = rmses)) +
  geom_point(color = "blue") +
  ggtitle("RMSEs vs. Lambdas--Movie based") +
  xlab("Lambdas") +
  ylab("RMSEs")

```
The regularization model considering only the movie effect did not lead to an improvement in RMSE. Let's re-run the model by including the user effect to assess its impact.

###Regularization Movie+user based model###

```{r regularizaion movie+user}
rmses <- sapply(lambdas, function(lambda) {
    # Calculate the average by user
    b_i <- edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + lambda))
  
   b_u <- edx %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu) / (n() + lambda))
  
  # predicted ratings on validation dataset
    predicted_ratings <- validation %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  # Predict the RMSE on the validation set
    return(RMSE(validation$rating, predicted_ratings))
})

# lambda value that minimizes the RMSE
min_lambda <- lambdas[which.min(rmses)]

# Predict RMSE on validation set
rmse_regularized_movie_user <- min(rmses)
rmse_results <- rmse_results %>% add_row(Model="Regularization Movie+User Based Model", RMSE=rmse_regularized_movie_user)
rmse_results

ggplot(data = data.frame(lambdas, rmses), aes(x = lambdas, y = rmses)) +
  geom_point(color = "blue") +
  ggtitle("RMSEs vs. Lambdas--Movie+user based") +
  xlab("Lambdas") +
  ylab("RMSEs")

```
By incorporating the user effect into the movie-only regularization model, we achieved the lowest RMSE to date. Next, let's explore the impact of adding genre as an additional factor to the model.

###MOvie+user+genre based model###
```{r regularization Movie+user+genre}
# Compute the predicted ratings on validation dataset using different values of lambda
rmses <- sapply(lambdas, function(lambda) {
  # Calculate the average by user
  b_i <- edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + lambda))
  
  # Calculate the average by user
  b_u <- edx %>%
    left_join(b_i, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu) / (n() + lambda))
  
  b_u_g <- edx %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(genres) %>%
    summarize(b_u_g = sum(rating - b_i - mu - b_u) / (n() + lambda))
  
  #predicted ratings on validation dataset
  predicted_ratings <- validation %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_u_g, by='genres') %>%
    mutate(pred = mu + b_i + b_u + b_u_g) %>%
    pull(pred)
    # Predict RMSE on validation set
    return(RMSE(validation$rating, predicted_ratings))
})

# lambda value that minimizes the RMSE
min_lambda <- lambdas[which.min(rmses)]
# Predict RMSE on validation set
rmse_regularized_movie_user_genre <- min(rmses)

rmse_results <- rmse_results %>% add_row(Model="Regularization Movie+User+Genre Based Model", RMSE=rmse_regularized_movie_user_genre)
rmse_results

ggplot(data = data.frame(lambdas, rmses), aes(x = lambdas, y = rmses)) +
  geom_point(color = "blue") +
  ggtitle("RMSEs vs. Lambdas--MOvie+user+genre based") +
  xlab("Lambdas") +
  ylab("RMSEs")

```
This model produced the lowest RMSE, 0.86445.


## Conclusion
The final model: 
$$
Y_{u,i} = \mu + b_i + b_u + +b_g+\epsilon_{u,i}
$$
With the following regularization techniques:



Bias Calculation:

$$
b_i = \frac{\sum_{u} \left( r_{u,i} - \mu \right)}{n_i + \lambda} 
$$

$$
   b_u = \frac{\sum_{i} \left( r_{u,i} - b_i - \mu \right)}{n_u + \lambda}
$$

$$
   b_g = \frac{\sum_{u} \left( r_{u,i} - b_i - \mu - b_u \right)}{n_g + \lambda}
$$

Predicted Ratings:
$$
\hat{r}_{u,i} = \mu + b_i + b_u + b_g
$$

RMSE calculation:
$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{u,i} \left( r_{u,i} - \hat{r}_{u,i} \right)^2}
$$

We developed seven models, ranging from simple to more complex approaches incorporating regularization techniques. The best-performing model was the regularized version that accounted for movie, user, and genre effects, yielding the lowest RMSE of 0.86445. However, additional variables, such as user demographic characteristics, may be missing and could further enhance the model's performance by potentially reducing the RMSE even further.



